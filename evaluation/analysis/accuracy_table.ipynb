{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders import load_dataset, load_inference_data, promptTechList, modelList, accuracy, problem_list\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "def evaluate_problem(problem : str) -> dict[str, dict[str, list[int]]]:\n",
    "\n",
    "    # formate the problem and its name\n",
    "    problem_name = problem\n",
    "    match problem:\n",
    "        case 'Direct Boolean Computation'|'DirectBooleanComputation':\n",
    "            from DirectBooleanComputation import response_evaluator\n",
    "            problem = 'DirectBooleanComputation'\n",
    "            problem_name = 'Direct Boolean Computation'\n",
    "\n",
    "        case 'Indirect Boolean Computation'|'IndirectBooleanComputation':\n",
    "            from IndirectBooleanComputation import response_evaluator\n",
    "            problem = 'IndirectBooleanComputation'\n",
    "            problem_name = 'Indirect Boolean Computation'\n",
    "        \n",
    "        case 'SAT':\n",
    "            from SAT import response_evaluator\n",
    "        \n",
    "        case 'SAT Count'|'SATCount':\n",
    "            from SATCount import response_evaluator\n",
    "            problem = 'SATCount'\n",
    "            problem_name = 'SAT Count'\n",
    "        \n",
    "        case 'TautologyQ':\n",
    "            from TautologyQ import response_evaluator\n",
    "        \n",
    "        case 'EquivalentQ':\n",
    "            from EquivalentQ import response_evaluator\n",
    "        \n",
    "        case 'CNF':\n",
    "            from CNF import response_evaluator\n",
    "        \n",
    "        case 'DNF':\n",
    "            from DNF import response_evaluator\n",
    "\n",
    "    # load corresponding dataset\n",
    "    dataset = load_dataset(problem)\n",
    "\n",
    "    inference_data = {\n",
    "        model: {p: load_inference_data(\n",
    "            problem, p, model) for p in promptTechList}\n",
    "        for model in modelList\n",
    "    }\n",
    "    \n",
    "    # evaluate all the reponses\n",
    "    evaluation_result = {\n",
    "        model : {\n",
    "            p: [\n",
    "                response_evaluator(response['response'], dataObject)\n",
    "                for dataObject, response in zip(dataset, inference_data[model][p])\n",
    "            ]\n",
    "        for p in promptTechList}\n",
    "    for model in modelList\n",
    "    }\n",
    "\n",
    "    return evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('llama2-13b', 'Zero-shot', 'DirectBooleanComputation'): 0.1520018778083294,\n",
       " ('llama2-13b',\n",
       "  'Zero-shot',\n",
       "  'IndirectBooleanComputation'): 0.08919597989949749,\n",
       " ('llama2-13b', 'Zero-shot', 'CNF'): 0.03752068602377012,\n",
       " ('llama2-13b', 'Zero-shot', 'DNF'): 0.03547465021814352,\n",
       " ('llama2-13b', 'Zero-shot', 'TautologyQ'): 0.511555215641938,\n",
       " ('llama2-13b', 'Zero-shot', 'EquivalentQ'): 0.22043265435022896,\n",
       " ('llama2-13b', 'Zero-shot', 'SAT'): 0.04887133545258967,\n",
       " ('llama2-13b', 'Zero-shot', 'SAT Count'): 0.05452247698140458,\n",
       " ('wizardmath-13b',\n",
       "  'Zero-shot',\n",
       "  'DirectBooleanComputation'): 0.08423311649118101,\n",
       " ('wizardmath-13b',\n",
       "  'Zero-shot',\n",
       "  'IndirectBooleanComputation'): 0.01870064608758076,\n",
       " ('wizardmath-13b', 'Zero-shot', 'CNF'): 0.04083044982698962,\n",
       " ('wizardmath-13b', 'Zero-shot', 'DNF'): 0.04068000601775237,\n",
       " ('wizardmath-13b', 'Zero-shot', 'TautologyQ'): 0.35886010707278954,\n",
       " ('wizardmath-13b', 'Zero-shot', 'EquivalentQ'): 0.5013027001421128,\n",
       " ('wizardmath-13b', 'Zero-shot', 'SAT'): 0.10143531451690589,\n",
       " ('wizardmath-13b', 'Zero-shot', 'SAT Count'): 0.004964795089366311,\n",
       " ('llama2-13b', 'Few-shot', 'DirectBooleanComputation'): 0.5595533498759305,\n",
       " ('llama2-13b', 'Few-shot', 'IndirectBooleanComputation'): 0.5264178033022254,\n",
       " ('llama2-13b', 'Few-shot', 'CNF'): 0.03824281630810892,\n",
       " ('llama2-13b', 'Few-shot', 'DNF'): 0.019136452534978185,\n",
       " ('llama2-13b', 'Few-shot', 'TautologyQ'): 0.5024440528048415,\n",
       " ('llama2-13b', 'Few-shot', 'EquivalentQ'): 0.5846755092373283,\n",
       " ('llama2-13b', 'Few-shot', 'SAT'): 0.6389557846064926,\n",
       " ('llama2-13b', 'Few-shot', 'SAT Count'): 0.19642534753565627,\n",
       " ('wizardmath-13b',\n",
       "  'Few-shot',\n",
       "  'DirectBooleanComputation'): 0.4986587083361277,\n",
       " ('wizardmath-13b',\n",
       "  'Few-shot',\n",
       "  'IndirectBooleanComputation'): 0.17641780330222542,\n",
       " ('wizardmath-13b', 'Few-shot', 'CNF'): 0.034511809839025125,\n",
       " ('wizardmath-13b', 'Few-shot', 'DNF'): 0.02608695652173913,\n",
       " ('wizardmath-13b', 'Few-shot', 'TautologyQ'): 0.4549928507298906,\n",
       " ('wizardmath-13b', 'Few-shot', 'EquivalentQ'): 0.5512000631612191,\n",
       " ('wizardmath-13b', 'Few-shot', 'SAT'): 0.27662717143499344,\n",
       " ('wizardmath-13b', 'Few-shot', 'SAT Count'): 0.181711500270807,\n",
       " ('llama2-13b', 'LtM', 'DirectBooleanComputation'): 0.15602575279994635,\n",
       " ('llama2-13b', 'LtM', 'IndirectBooleanComputation'): 0.17433596554199568,\n",
       " ('llama2-13b', 'LtM', 'CNF'): 0.043448172107717765,\n",
       " ('llama2-13b', 'LtM', 'DNF'): 0.04362870467880247,\n",
       " ('llama2-13b', 'LtM', 'TautologyQ'): 0.33392079273767167,\n",
       " ('llama2-13b', 'LtM', 'EquivalentQ'): 0.07828043581241118,\n",
       " ('llama2-13b', 'LtM', 'SAT'): 0.08582988151430498,\n",
       " ('llama2-13b', 'LtM', 'SAT Count'): 0.0927965336703376,\n",
       " ('wizardmath-13b', 'LtM', 'DirectBooleanComputation'): 0.19234122459928912,\n",
       " ('wizardmath-13b', 'LtM', 'IndirectBooleanComputation'): 0.06575735821966978,\n",
       " ('wizardmath-13b', 'LtM', 'CNF'): 0.04212426658642997,\n",
       " ('wizardmath-13b', 'LtM', 'DNF'): 0.04164284639687077,\n",
       " ('wizardmath-13b', 'LtM', 'TautologyQ'): 0.08821866790809031,\n",
       " ('wizardmath-13b', 'LtM', 'EquivalentQ'): 0.05692404863413864,\n",
       " ('wizardmath-13b', 'LtM', 'SAT'): 0.033811771505635295,\n",
       " ('wizardmath-13b', 'LtM', 'SAT Count'): 0.00839501715111031,\n",
       " ('llama2-13b', 'CoT', 'DirectBooleanComputation'): 0.2070283683186909,\n",
       " ('llama2-13b', 'CoT', 'IndirectBooleanComputation'): 0.21974156496769562,\n",
       " ('llama2-13b', 'CoT', 'CNF'): 0.04089062735068452,\n",
       " ('llama2-13b', 'CoT', 'DNF'): 0.04055965097036257,\n",
       " ('llama2-13b', 'CoT', 'TautologyQ'): 0.30479167359425396,\n",
       " ('llama2-13b', 'CoT', 'EquivalentQ'): 0.08499131533238591,\n",
       " ('llama2-13b', 'CoT', 'SAT'): 0.06572905628873262,\n",
       " ('llama2-13b', 'CoT', 'SAT Count'): 0.10101101281819823,\n",
       " ('wizardmath-13b', 'CoT', 'DirectBooleanComputation'): 0.19888002146066663,\n",
       " ('wizardmath-13b', 'CoT', 'IndirectBooleanComputation'): 0.0624551328068916,\n",
       " ('wizardmath-13b', 'CoT', 'CNF'): 0.042064089062735066,\n",
       " ('wizardmath-13b', 'CoT', 'DNF'): 0.04236497668120957,\n",
       " ('wizardmath-13b', 'CoT', 'TautologyQ'): 0.08389585342333655,\n",
       " ('wizardmath-13b', 'CoT', 'EquivalentQ'): 0.0446865624506553,\n",
       " ('wizardmath-13b', 'CoT', 'SAT'): 0.029027389782615678,\n",
       " ('wizardmath-13b', 'CoT', 'SAT Count'): 0.008304748149485466,\n",
       " ('llama2-13b',\n",
       "  'Few-shot-CoT',\n",
       "  'DirectBooleanComputation'): 0.49245523439071825,\n",
       " ('llama2-13b',\n",
       "  'Few-shot-CoT',\n",
       "  'IndirectBooleanComputation'): 0.4277099784637473,\n",
       " ('llama2-13b', 'Few-shot-CoT', 'CNF'): 0.09682563562509403,\n",
       " ('llama2-13b', 'Few-shot-CoT', 'DNF'): 0.13961185497216788,\n",
       " ('llama2-13b', 'Few-shot-CoT', 'TautologyQ'): 0.4761413892860706,\n",
       " ('llama2-13b', 'Few-shot-CoT', 'EquivalentQ'): 0.33601768514132324,\n",
       " ('llama2-13b', 'Few-shot-CoT', 'SAT'): 0.3589891789487204,\n",
       " ('llama2-13b', 'Few-shot-CoT', 'SAT Count'): 0.07699945838599025,\n",
       " ('wizardmath-13b',\n",
       "  'Few-shot-CoT',\n",
       "  'DirectBooleanComputation'): 0.4048688887398565,\n",
       " ('wizardmath-13b',\n",
       "  'Few-shot-CoT',\n",
       "  'IndirectBooleanComputation'): 0.23334529791816225,\n",
       " ('wizardmath-13b', 'Few-shot-CoT', 'CNF'): 0.0771776741387092,\n",
       " ('wizardmath-13b', 'Few-shot-CoT', 'DNF'): 0.16380321949751767,\n",
       " ('wizardmath-13b', 'Few-shot-CoT', 'TautologyQ'): 0.18711136235161108,\n",
       " ('wizardmath-13b', 'Few-shot-CoT', 'EquivalentQ'): 0.20456339807358281,\n",
       " ('wizardmath-13b', 'Few-shot-CoT', 'SAT'): 0.2107696753684616,\n",
       " ('wizardmath-13b', 'Few-shot-CoT', 'SAT Count'): 0.026990431485827766}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_result = {(model, p, problem): accuracy(evaluate_problem(problem)[model][p])\n",
    "                    for p in promptTechList\n",
    "                    for model in modelList\n",
    "                    for problem in problem_list\n",
    "                    }\n",
    "accuracy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  & 15.2 / 8.42    & 8.92 / 1.87    & 3.75 / 4.08    & 3.55 / 4.07    & 51.16 / 35.89    & 22.04 / 50.13    & 4.89 / 10.14    & 5.45 / 0.5  ',\n",
       " '  & 55.96 / 49.87    & 52.64 / 17.64    & 3.82 / 3.45    & 1.91 / 2.61    & 50.24 / 45.5    & 58.47 / 55.12    & 63.9 / 27.66    & 19.64 / 18.17  ',\n",
       " '  & 15.6 / 19.23    & 17.43 / 6.58    & 4.34 / 4.21    & 4.36 / 4.16    & 33.39 / 8.82    & 7.83 / 5.69    & 8.58 / 3.38    & 9.28 / 0.84  ',\n",
       " '  & 20.7 / 19.89    & 21.97 / 6.25    & 4.09 / 4.21    & 4.06 / 4.24    & 30.48 / 8.39    & 8.5 / 4.47    & 6.57 / 2.9    & 10.1 / 0.83  ',\n",
       " '  & 49.25 / 40.49    & 42.77 / 23.33    & 9.68 / 7.72    & 13.96 / 16.38    & 47.61 / 18.71    & 33.6 / 20.46    & 35.9 / 21.08    & 7.7 / 2.7  ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    str([\n",
    "        f\"& {round(accuracy_result[('llama2-13b', p, problem)] * 100, 2)} / {round(accuracy_result[(('wizardmath-13b', p, problem))] * 100, 2)}\" \n",
    "        for problem in problem_list\n",
    "        ]).replace('[',' ').replace(']',' ').replace('\\'',' ').replace(',',' ')\n",
    "    for p in promptTechList\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
